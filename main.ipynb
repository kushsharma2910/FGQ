{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushsharma2910/FGQ/blob/kush/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Ofmk-G-Nk4",
        "colab_type": "code",
        "outputId": "3fe7c2c2-5d13-4a5f-c847-c1de44853f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32BpqMho9hU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDjFHyaS9hVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################################################################\n",
        "################################# CREATING LARGEDF  ###################################\n",
        "#######################################################################################\n",
        "\n",
        "def change_index_to_time(df):\n",
        "    if \"Time (UTC)\" in df.columns:\n",
        "      df['Time (UTC)'] = pd.to_datetime(df['Time (UTC)'])\n",
        "      df.set_index('Time (UTC)', inplace = True, drop = True)\n",
        "    \n",
        "#Finds the dataframe with largest number of time stamps and returns an empty dataframe with same index\n",
        "def create_empty_df(dir_name):\n",
        "    largest_timeframe = 0\n",
        "    path_to_largest = \"\"\n",
        "\n",
        "    for asset_class in os.listdir(dir_name):\n",
        "        for asset in os.listdir(dir_name +  \"/\" + asset_class):    \n",
        "            df = pd.read_csv(dir_name +  \"/\" + asset_class + \"/\" + asset)\n",
        "            if df.shape[0] > largest_timeframe:\n",
        "                largest_timeframe = df.shape[0]\n",
        "                path_to_largest = dir_name +  \"/\" + asset_class + \"/\" + asset\n",
        "\n",
        "    largest_dataframe = pd.read_csv(path_to_largest)\n",
        "    change_index_to_time(largest_dataframe)\n",
        "    largest_dataframe = pd.DataFrame(index = largest_dataframe.index)\n",
        "    return largest_dataframe\n",
        "\n",
        "#######################################################################################\n",
        "################################# FEATURE BUILDING  ###################################\n",
        "#######################################################################################\n",
        "\n",
        "def trim_fft(df):\n",
        "    all_fft = []\n",
        "    for column in df.columns:\n",
        "        if \"fft\" in column:\n",
        "            all_fft.append(column)\n",
        "    df = df.drop(columns = all_fft)\n",
        "    return df\n",
        "\n",
        "def fill_large_df(dir_name, large_df):\n",
        "    asset67 = []\n",
        "    for asset_class in os.listdir(dir_name):\n",
        "        for asset in os.listdir(dir_name +  \"/\" + asset_class): \n",
        "            extract_name = asset.split(\"_\")\n",
        "            asset67.append(extract_name[0][:-4])\n",
        "            df = pd.read_csv(dir_name +  \"/\" + asset_class + \"/\" + asset)\n",
        "            change_index_to_time(df)\n",
        "            df = trim_fft(df)\n",
        "            large_df = large_df.join(df)\n",
        "            rename_dic = {}\n",
        "            for column in df.columns:    \n",
        "                rename_dic[column] = column + \"@\" + extract_name[0][:-4]\n",
        "            large_df = large_df.rename(columns = rename_dic) \n",
        "            \n",
        "    return large_df, asset67\n",
        "\n",
        "def add_lag_features(only_feat_df):\n",
        "  lag_feat = only_feat_df.copy()\n",
        "  BASE_FEATURES = [\"Open@\" + target_asset, \"High@\" + target_asset, \"Low@\" + target_asset, \"Close@\" + target_asset]\n",
        "  N_WINDOW = [4, 24, 128, 256]\n",
        "  prevlag = 1 \n",
        "  for window in N_WINDOW:\n",
        "    rolled = lag_feat[BASE_FEATURES].shift(prevlag).rolling(window=window)\n",
        "    lag_feat = lag_feat.join(rolled.mean().add_suffix(f'_window_{window}_mean'))\n",
        "    lag_feat = lag_feat.join(rolled.max().add_suffix(f'_window_{window}_max'))\n",
        "    lag_feat = lag_feat.join(rolled.min().add_suffix(f'_window_{window}_min'))\n",
        "    lag_feat = lag_feat.join(rolled.std().add_suffix(f'_window_{window}_std'))\n",
        "  return lag_feat\n",
        "\n",
        "def remove_spread(df, target_asset):\n",
        "    target_asset_spread = []\n",
        "    features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume \"]\n",
        "\n",
        "    for feature in features:\n",
        "        target_asset_spread.append(feature + \"_spread@\" + target_asset)\n",
        "\n",
        "    remove_columns = []\n",
        "    for column in only_feat_df.columns:\n",
        "        if (column not in target_asset_spread) and (\"spread\" in column):\n",
        "            remove_columns.append(column)\n",
        "\n",
        "    df = df.drop(columns = remove_columns)\n",
        "    return df\n",
        "\n",
        "def extract_features(large_df, target_asset, target_window, asset67, add_lag = False, keep_spread = True):\n",
        "    df = large_df.copy()\n",
        "\n",
        "    #remove the spread columns of assets\n",
        "    #other than the target asset\n",
        "    if keep_spread == False:\n",
        "        df = remove_spread(df, target_asset)\n",
        "\n",
        "    #comment out this line if you \n",
        "    #don't want to use the lag features\n",
        "    if add_lag == True:\n",
        "      df = add_lag_features(df)\n",
        "\n",
        "    df[\"H-\" + str(target_window) + \"hr\" ] = df.pop(\"H-\" + str(target_window) + \"hr@\" + target_asset)\n",
        "    df[\"L-\" + str(target_window) + \"hr\" ] = df.pop(\"L-\" + str(target_window) + \"hr@\" + target_asset)\n",
        "    \n",
        "    #removing targets of assets other\n",
        "    # than the target assset\n",
        "    for asset in asset67:\n",
        "        if asset != target_asset:\n",
        "            df = df.drop(columns = [\"H-4hr@\" + asset, \"L-4hr@\" + asset, \"H-12hr@\" + asset,\n",
        "                               \"L-12hr@\" + asset, \"H-24hr@\" + asset,\"L-24hr@\" + asset])\n",
        "            \n",
        "    #this loop asserts that labels for target asset is\n",
        "    #not present for other windows\n",
        "    windows = [\"4\", \"12\", \"24\"]\n",
        "    for window in windows:\n",
        "      if window != str(target_window):\n",
        "        high = \"H-\" + window + \"hr@\" + target_asset\n",
        "        low = \"L-\" + window + \"hr@\" + target_asset\n",
        "        if high in df.columns:\n",
        "          df = df.drop(columns = [high])\n",
        "        if low in df.columns:\n",
        "          df = df.drop(columns = [low])\n",
        "\n",
        "    change_index_to_time(df)        \n",
        "    return df\n",
        "\n",
        "def normalize_df(df):\n",
        "    result = df.copy()\n",
        "    for feature_name in df.columns:\n",
        "        max_value = df[feature_name].max()\n",
        "        min_value = df[feature_name].min()\n",
        "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
        "    return result\n",
        "\n",
        "#######################################################################################\n",
        "################################# MODEL BUILDING  #####################################\n",
        "#######################################################################################\n",
        "\n",
        "\n",
        "#predicting the H as previous value\n",
        "def create_baseline(only_feat_df, direction, target_window):     \n",
        "    preds = np.array(pd.Series(only_feat_df[direction + \"-\" + str(target_window) + \"hr\"]).shift(1).bfill(axis = 0))\n",
        "    actual = np.array(only_feat_df[direction + \"-\" + str(target_window) + \"hr\"])\n",
        "    return preds, actual\n",
        "\n",
        "#######################################################################################\n",
        "############################# ACCURACY CALCULATIONS  ##################################\n",
        "#######################################################################################\n",
        "\n",
        "def acc_abs(preds, actual, tolerance):\n",
        "    tolerance /= 100\n",
        "    return np.sum(np.abs(preds/actual-1) <= tolerance)\n",
        "\n",
        "def acc_dir(preds, actual, tolerance, direction):\n",
        "    tolerance /= 100\n",
        "    score = 0\n",
        "    for ix in range(preds.shape[0]):\n",
        "        if actual[ix] > preds[ix]*(1+tolerance)  and direction == \"H\":\n",
        "            score += 1\n",
        "        elif actual[ix] < preds[ix]*(1-tolerance)  and direction == \"L\":\n",
        "             score += 1\n",
        "    return score\n",
        "\n",
        "def min_pips(preds, actual, opens, tolerance, pips, direction):\n",
        "    pips /= 100\n",
        "    tolerance /= 100\n",
        "    score = 0\n",
        "    for ix in range(preds.shape[0]):\n",
        "        if preds[ix] < actual[ix]*(1+tolerance)  and (direction == \"H\" and (actual[ix] - opens[ix])>=pips):\n",
        "            score += 1 \n",
        "        elif preds[ix] > actual[ix]*(1+tolerance)  and (direction == \"L\" and (opens[ix] - actual[ix])>=pips):\n",
        "             score += 1\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ox-Y7o39hVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#name of the root directory which contains asset classes which further contain asset csv ask and bid pairs\n",
        "# dir_name = \"Data\"\n",
        "# large_df = create_empty_df(dir_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL0w8A809hVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#asset67 contains the 67 selected assets which have data for more than 6 years\n",
        "# large_df_, asset67 = fill_large_df(dir_name, large_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lGdKz6O9hVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Filling in previous values for values in middle\n",
        "# large_df_ = large_df_.ffill(axis=0)\n",
        "\n",
        "#Filling in next value for initial nan values\n",
        "#This is used for assets which do not have initial data\n",
        "# large_df_ = large_df_.bfill(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vztj-_QJ_Kah",
        "colab_type": "code",
        "outputId": "65407711-9409-4ba2-fa0b-329887739abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#to extract to a given path\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"drive/My Drive/large_df_.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"drive/My Drive/\")\n",
        "\n",
        "#to extract to the home dir\n",
        "!unzip \"drive/My Drive/large_df_.zip\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/large_df_.zip\n",
            "replace large_df_.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVeRVJ1SCbm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#When using google drive\n",
        "large_df_ = pd.read_csv(\"large_df_.csv\")\n",
        "\n",
        "#List of assets present\n",
        "#in large_df_\n",
        "asset67 = np.array(pd.read_csv(\"drive/My Drive/asset67.csv\").iloc[:, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDBBD8Sn9hVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input variables\n",
        "target_asset = \"XAGUSD\"\n",
        "target_window = 12\n",
        "direction = \"L\"\n",
        "#give tolerance in percent\n",
        "tolerance = 0.04\n",
        "\n",
        "only_feat_df = extract_features(large_df_, target_asset, target_window, asset67, add_lag=False, keep_spread = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q93nvpzN9hVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#in the xlsx file, first row and columns have been removed for efficient reading\n",
        "pip_xl = pd.read_excel(\"drive/My Drive/Labels to Forecast.xlsx\")\n",
        "\n",
        "#querying pips\n",
        "pips = pip_xl[pip_xl[\"Label\"] == target_asset][str(target_window) + \" Hour\"].values[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57UvZ4hIqzXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preds, actual = create_baseline(only_feat_df, direction, target_window)\n",
        "\n",
        "# #calculate accuracies\n",
        "# print(acc_abs(preds, actual, tolerance))\n",
        "# print(acc_dir(preds, actual, tolerance, direction))\n",
        "# print(min_pips(preds, actual, only_feat_df[\"Open@\" + target_asset], tolerance, pips, direction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA2iL8KQkUJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# norm_feat = normalize_df(only_feat_df.iloc[:, :-2])\n",
        "# target = only_feat_df[direction + \"-\" + str(target_window) + \"hr\"]\n",
        "\n",
        "only_norm = normalize_df(only_feat_df.iloc[:, :-2])\n",
        "only_norm[direction + \"-\" + str(target_window) + \"hr\"] = only_feat_df[direction + \"-\" + str(target_window) + \"hr\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FdLD68SEu3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "import pickle\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import logging\n",
        "\n",
        "params = {\n",
        "    \"batch_size\": 256, \n",
        "    \"epochs\": 300,\n",
        "    \"lr\": 0.00010000,\n",
        "    \"time_steps\": 512\n",
        "}\n",
        "\n",
        "iter_changes = \"dropout_layers_0.4_0.4\"\n",
        "\n",
        "TIME_STEPS = params[\"time_steps\"]\n",
        "BATCH_SIZE = params[\"batch_size\"]\n",
        "\n",
        "def trim_dataset(mat,batch_size):\n",
        "    no_of_rows_drop = mat.shape[0]%batch_size\n",
        "    if no_of_rows_drop > 0:\n",
        "        return mat[:-no_of_rows_drop]\n",
        "    else:\n",
        "        return mat\n",
        "\n",
        "def build_timeseries(mat, y_col_index = -1):\n",
        "    dim_0 = mat.shape[0] - TIME_STEPS\n",
        "    dim_1 = mat.shape[1]\n",
        "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
        "    y = np.zeros((dim_0,))\n",
        "    for i in tqdm_notebook(range(dim_0)):\n",
        "        x[i] = mat[i:TIME_STEPS+i]\n",
        "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
        "    return x, y\n",
        "\n",
        "def create_model():\n",
        "    lstm_model = Sequential()\n",
        "    # (batch_size, timesteps, data_dim)\n",
        "    lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),\n",
        "                        dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
        "                        kernel_initializer='random_uniform'))\n",
        "    lstm_model.add(Dropout(0.4))\n",
        "    lstm_model.add(LSTM(60, dropout=0.0))\n",
        "    lstm_model.add(Dropout(0.4))\n",
        "    lstm_model.add(Dense(20,activation='relu'))\n",
        "    lstm_model.add(Dense(1,activation='sigmoid'))\n",
        "    optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
        "    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "    return lstm_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtkdSnJHHMNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, df_test = train_test_split(only_norm, train_size=0.8, test_size=0.2, shuffle=False)\n",
        "print(\"Train--Test size\", len(df_train), len(df_test))\n",
        "\n",
        "x_t, y_t = build_timeseries(df_train, -1)\n",
        "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
        "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
        "print(\"Batch trimmed size\",x_t.shape, y_t.shape)\n",
        "\n",
        "x_temp, y_temp = build_timeseries(x_test, -1)\n",
        "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
        "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
        "print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
        "y_pred = y_pred.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfqhA0c5VQB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}